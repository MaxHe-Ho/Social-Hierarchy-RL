{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyObaVD2/50lO4dG/MeePLgg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neven-x/Social-Hierarchy-RL/blob/main/RL_Social_Hierarchy_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jmu7fw-JNem8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import gym\n",
        "from gym import spaces\n",
        "\n",
        "import random\n",
        "from collections import deque\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GridWorld(gym.Env, grid_size=10, num_agents=10):\n",
        "    def __init__(self, grid_size, num_agents):\n",
        "        super(GridWorld, self).__init__()\n",
        "        self.grid_size = grid_size\n",
        "        self.num_tiles = self.grid_size * self.grid_size\n",
        "\n",
        "        self.agent_positions = {}\n",
        "        for agent_id in range(num_agents):\n",
        "\n",
        "            agent_position = np.random.randint(0, grid_size, 2)\n",
        "            self.agent_positions[agent_id] = agent_position\n",
        "\n",
        "            agent_position_map = np.zeros((self.gridsize, self.gridsize))\n",
        "            agent_position_map[agent_position] = 1\n",
        "\n",
        "        food_position = np.random.randint(0, grid_size, 2)\n",
        "        self.grid_food = np.zeros((self.grid_size, self.grid_size), dtype=np.float32)\n",
        "        self.grid_food[food_position] = 1\n",
        "\n",
        "        #self.agent_position = np.random.randint(0, grid_size+1, 2)\n",
        "\n",
        "        # Define action space and observation space\n",
        "        self.action_space = spaces.Discrete(4)  # Up, Down, Left, Right\n",
        "        self.observation_space = spaces.Box(low=0, high=1, shape=(self.grid_size, self.grid_size, num_agents + 1), dtype=np.float32)\n",
        "\n",
        "    def reset(self):\n",
        "        # Reset the environment to the initial state\n",
        "        self.grid_agent = np.zeros((self.grid_size, self.grid_size), dtype=np.float32)\n",
        "        self.grid_food = np.zeros((self.grid_size, self.grid_size), dtype=np.float32)\n",
        "        self.opponents = np.zeros((self.grid_size, self.grid_size), dtype=np.float32)\n",
        "        self.agent_position = np.random.randint(0, grid_size+1, 2)\n",
        "\n",
        "        return self._get_observation()\n",
        "\n",
        "    def step(self, agent_id, action):\n",
        "        # Take an action in the environment\n",
        "        self._move_agent(action)\n",
        "\n",
        "        # Calculate the reward\n",
        "        reward = self._calculate_reward()\n",
        "\n",
        "        # Check if episode is done (e.g., agent reaches goal, runs out of oxygen, etc.)\n",
        "        done = self._is_episode_done()\n",
        "\n",
        "        return self._get_observation(), reward, done, {}\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        # Visualize the environment state (optional)\n",
        "        # Here, we print the grid with agent's position, food, and other agent\n",
        "        print(f\"Agent Position:\\n{self.grid_agent}\\n\")\n",
        "        print(f\"Food:\\n{self.grid_food}\\n\")\n",
        "        print(f\"opponents:\\n{self.opponents}\\n\")\n",
        "\n",
        "    def close(self):\n",
        "        # Perform any necessary cleanup or finalization steps (optional)\n",
        "        pass\n",
        "\n",
        "    def _move_agent(self, action):\n",
        "        # Move the agent based on the selected action\n",
        "        x, y = self.agent_position\n",
        "\n",
        "        if action == 0:  # Up\n",
        "            x -= 1\n",
        "        elif action == 1:  # Down\n",
        "            x += 1\n",
        "        elif action == 2:  # Left\n",
        "            y -= 1\n",
        "        elif action == 3:  # Right\n",
        "            y += 1\n",
        "\n",
        "        # Check if the new position is within grid boundaries\n",
        "        if 0 <= x < self.grid_size and 0 <= y < self.grid_size:\n",
        "            self.agent_positions[agent_id] = (x, y)\n",
        "\n",
        "    def two_on_food_tile(self):\n",
        "        food_tiles = np.where(self.grid_food == 1)\n",
        "\n",
        "        for food_tile in food_tiles:\n",
        "          overlaps = np.array(self.agent_positions.values()) == food_tile\n",
        "\n",
        "\n",
        "    def _calculate_reward(self):\n",
        "        # Calculate the reward based on the state of the environment\n",
        "        return 0  # Update with your reward calculation logic\n",
        "\n",
        "    def _is_episode_done(self):\n",
        "        # Check if the episode is done based on some termination conditions\n",
        "        return False  # Update with your termination condition logic\n",
        "\n",
        "    def _get_observation(self):\n",
        "        # Get the current observation and stack the arrays together\n",
        "        observation = np.stack((self.grid_agent, self.grid_food, self.opponents), axis=2)\n",
        "        return observation\n",
        "\n",
        "\n",
        "gym.register(\n",
        "    id='GridWorld',\n",
        "    entry_point=GridWorld\n",
        ")"
      ],
      "metadata": {
        "id": "O5LJomvrNkXh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, state_shape, action_size):\n",
        "        def __init__(self, state_shape, action_size):\n",
        "        self.state_shape = state_shape\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=2000)\n",
        "        self.gamma = 0.95  # discount rate\n",
        "        self.epsilon = 1.0  # exploration rate\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.learning_rate = 0.001\n",
        "        self.model = self._build_model()\n",
        "\n",
        "        self.food = 50\n",
        "\n",
        "    def _build_model(self):\n",
        "        model = Sequential()\n",
        "        model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=self.state_shape))\n",
        "        model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(128, activation='relu'))\n",
        "        model.add(Dense(self.action_size, activation='linear'))\n",
        "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
        "        return model\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        act_values = self.model.predict(state)\n",
        "        return np.argmax(act_values[0])\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            target = reward\n",
        "            if not done:\n",
        "                target = (reward + self.gamma *\n",
        "                          np.amax(self.model.predict(next_state)[0]))\n",
        "            target_f = self.model.predict(state)\n",
        "            target_f[0][action] = target\n",
        "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def load(self, name):\n",
        "        self.model.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.model.save_weights(name)\n"
      ],
      "metadata": {
        "id": "AI6-NSubtqn5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('GridWorld')\n",
        "\n",
        "n_actions = env.action_space.n\n",
        "state_shape = (10, 10, 3)\n",
        "\n",
        "dqn = DQNAgent(state_shape, n_actions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zs5aN0qht1i3",
        "outputId": "61e640dc-f44f-46b6-cdb8-3e80840e770f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:31: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (10, 10)\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        }
      ]
    }
  ]
}